import os
import requests
import zipfile
import torch
import torchvision
from torchvision.datasets import CocoDetection
from torchvision.transforms import functional as F
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt

# URLs and paths for the COCO 2017 validation dataset
urls = {
    "val_images": "http://images.cocodataset.org/zips/val2017.zip",
    "annotations": "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
}
data_dir = "./data"
val_images_dir = os.path.join(data_dir, "val2017")
annotations_dir = os.path.join(data_dir, "annotations")
annotation_file = os.path.join(annotations_dir, "instances_val2017.json")

# Function to download and extract zip files
def download_and_extract(url, extract_to):
    local_filename = os.path.join(data_dir, url.split("/")[-1])
    if not os.path.exists(extract_to):
        print(f"Downloading {local_filename}...")
        with requests.get(url, stream=True) as r:
            with open(local_filename, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"Extracting {local_filename}...")
        with zipfile.ZipFile(local_filename, "r") as zip_ref:
            zip_ref.extractall(extract_to)
        os.remove(local_filename)

# Download dataset
download_and_extract(urls["val_images"], val_images_dir)
download_and_extract(urls["annotations"], annotations_dir)

# Load COCO dataset
def transform(image, target):
    image = F.to_tensor(image)
    if torch.rand(1).item() > 0.5:
        image = F.hflip(image)
    return image, target

dataset = CocoDetection(root=val_images_dir, annFile=annotation_file, transform=transform)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

# Load and modify Faster R-CNN model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
num_classes = len(dataset.coco.cats) + 1  # COCO classes + 1 for background
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train function
def train_model(model, train_loader, optimizer, num_epochs=3):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for images, targets in train_loader:
            images = list(image for image in images)
            targets = [{k: v for k, v in t.items()} for t in targets]
            optimizer.zero_grad()
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            losses.backward()
            optimizer.step()
            total_loss += losses.item()
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}")

train_model(model, train_loader, optimizer)

# Evaluation function with a sample display
def evaluate_model(model, test_loader):
    model.eval()
    with torch.no_grad():
        for images, targets in test_loader:
            images = list(image for image in images)
            outputs = model(images)
            img = images[0].permute(1, 2, 0).cpu().numpy()
            plt.figure(figsize=(10, 10))
            plt.imshow(img)
            for box in outputs[0]['boxes'].cpu().numpy():
                plt.gca().add_patch(
                    plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],
                                  fill=False, color='red', linewidth=2)
                )
            plt.axis("off")
            plt.show()
            break  # Show one batch only

# Evaluate and display results
evaluate_model(model, test_loader)
